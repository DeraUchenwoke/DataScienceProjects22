---
title: "Coursework MAP501 2022"
subtitle: 'Student ID: B928510'
output:
  pdf_document:
    toc: yes
  html_document:
    self_contained: yes
    highlight: textmate
    toc: yes
    number_sections: no
editor_options:
  markdown:
    wrap: 72
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  comment = "",
  results = "hold",
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  fig.width = 4,
  fig.height = 2.6,
  fig.align = "center"
)
```

You will submit your coursework in the form of a single R notebook (i.e.`.Rmd` file) which can be rendered ("knitted") to an `.pdf`
document. Specifically, submit on Learn:

-   your R notebook (i.e. the `.Rmd` file),
-   the rendered `.pdf` version of your notebook. You might find it easier to knit to html, then print the html file to a pdf.


The coursework will be marked on the basis of correctness of code,
interpretation of outputs and commentary as indicated. Therefore, please
ensure that all code and outputs are visible in the knit document.

# Preamble

```{r, message = FALSE}
library(readxl) # not in original coursework doc -> to read csv file.
library(here) # not in original coursework doc -> to get data file.
library(tidyverse) # not in original coursework doc.
library(janitor) # not in original coursework doc -> used to clean column names.
library(lindia)
library(rio)
library(dplyr)
library(tidyr)
library(magrittr)
library(ggplot2)
library(pROC)
library(car)
library(nnet)
library(caret)
library(lme4)
library(AmesHousing)
```

```{r}
Ames<-make_ames()
```

# 1. Data Preparation

a.  Import the soccer.csv dataset as "footballer_data". (2 points)

```{r}
# Read in data & clean column names.
footballer_data <- read_csv(here("data", "soccer.csv")) 
```

b. Ensure all character variables are treated as factors and where variable names have a space, rename the variables without these. (3 points)

```{r}
# Rename variables with spaces.
footballer_data <- footballer_data %>% 
  clean_names()

# Convert all character variables to factors.
footballer_data <- footballer_data %>% 
  mutate(across(
    c(full_name, birthday_gmt, position, current_club, nationality), as.factor))
footballer_data

```

c. Remove the columns birthday and birthday_GMT. (2 points)

```{r}
# Removes birthday column and birthday_gmt column.
footballer_data <- footballer_data %>% 
  select(-c(birthday, birthday_gmt))
footballer_data
```

d. Remove the cases with age\<=15 and age\>40. (2 points)

```{r}
footballer_data <- footballer_data %>% 
  filter(age > 15 & age <= 40)

# The minimum and maximum age was printed to check range was correct.
footballer_data %>% 
  summarise (
    min_age = min(age),
    max_age = max(age)
  )

footballer_data
```


# 2. Linear Regression

In this problem, you are going to investigate the response variable Total_Bsmt_SF in "Ames" dataset through linear regression.

a.  By adjusting x axis range and number of bars, create a useful histogram of Total_Bsmt_SF on the full dataset. Ensure that plot titles and axis labels are clear. (4 points)

```{r}
Ames 
# Where 'Total_Bsmt_SF' is 'Total square feet of basement area'.
# Since it is linear regression we want the histogram to be 'bell-shaped' 
# i.e., follow a Gaussian distribution.
# Plotting area as log(area) provided the most useful histogram...
# to linarize the data.
# Bin width was estimated via trial and error.

figure_1 <- Ames %>% 
  ggplot(aes(x = log(Total_Bsmt_SF))) +
  geom_histogram(colour = "black", fill = "white", binwidth = 0.25) +
  labs (
    subtitle = "Histogram of total square feet of basement area\nagainst frequency",
    x = "Total square feet of basement area, "~log(m^2),
    y = "Frequency"
  ) + 
  theme_classic() + 
  theme(
    panel.grid.major.x = element_blank(), 
    panel.grid.minor.x = element_blank(),
    panel.grid.major.y = element_line(colour = "grey80"), 
    panel.grid.minor.y = element_blank(),
    panel.background   = element_blank()
  )

figure_1

```

```{r}

# A frequency density plot was created.
# This was overlaid by a distribution curve to validate...
# the normal distribution of 'Total_Bsmt_SF'.

figure_2 <- Ames %>%
  ggplot(aes(x = log(Total_Bsmt_SF))) +
  geom_histogram(aes(y = ..density..), colour = 1, fill = "white", binwidth = 0.25) +
  geom_density(lwd = 0.75, colour = 4, fill = 4, alpha = 0.15, adjust = 2.75) +
  labs (
    subtitle = "Histogram of total square feet of basement area\nagainst frequency density",
    x = "Total square feet of basement area, "~log(m^2),
    y = "Frequency density"
  ) + 
  theme_classic() + 
  theme(
    panel.grid.major.x = element_blank(), 
    panel.grid.minor.x = element_blank(),
    panel.grid.major.y = element_line(colour = "grey80"), 
    panel.grid.minor.y = element_blank(),
    panel.background   = element_blank(),
  )

figure_2

```


b.  Using "Ames" dataset to create a new dataset called "Ames2" in which you remove all cases corresponding to:
  (i) MS_Zoning categories of A_agr (agricultural), C_all (commercial) and I_all (industrial),
  (ii) BsmtFin_Type_1 category of "No_Basement". 
  (iii) Bldg_Type category of "OneFam"
  
  and drop the unused levels from the dataset "Ames2". (4 points)
```{r}
# Checked levels before removal.
levels(Ames$MS_Zoning)
levels(Ames$BsmtFin_Type_1)
levels(Ames$Bldg_Type)
```
  
```{r}

# (i), (ii) and (iii)
Ames2 <- Ames %>%
  filter(
      MS_Zoning != "A_agr",
      MS_Zoning != "C_all",
      MS_Zoning != "I_all",
      BsmtFin_Type_1 != "No_Basement", 
      Bldg_Type !="OneFam" 
      ) 

Ames2$MS_Zoning <- droplevels(Ames2$MS_Zoning)
Ames2$BsmtFin_Type_1 <- droplevels(Ames2$BsmtFin_Type_1)
Ames2$Bldg_Type <- droplevels(Ames2$Bldg_Type)

# Checked levels after removal.
levels(Ames2$MS_Zoning)
levels(Ames2$BsmtFin_Type_1)
levels(Ames2$Bldg_Type)
```
  
c.  Choose an appropriate plot to investigate the relationship between
    Bldg_Type and Total_Bsmt_SF in Ames2. (2 points)
    
```{r}

# Box plot since the relationship is categorical-numerical.
figure_3 <- Ames2 %>% 
  ggplot(aes(x = Bldg_Type, y = Total_Bsmt_SF)) + 
  geom_boxplot() + 
  labs (
    subtitle = "Boxplot of the total basement area\nfor different dwelling types",
    x = "Type of dwelling", 
    y = "Total square ft of basement area"
  ) + 
  theme_classic() + 
  theme(
    panel.grid.major.x = element_blank(), 
    panel.grid.minor.x = element_blank(),
    panel.grid.major.y = element_line(colour = "grey80"), 
    panel.grid.minor.y = element_blank(),
    panel.background   = element_blank(),
  )
  

figure_3

```
    

d.  Choose an appropriate plot to investigate the relationship between
    Year_Built and Total_Bsmt_SF in Ames2. Color points according to the
    factor Bldg_Type. Ensure your plot has a clear title, axis labels
    and legend. What do you notice about how Basement size has changed
    over time? Were there any slowdowns in construction over this
    period? When? Can you think why? (4 points)
    
```{r}

Ames2 %>% 
  ggplot(aes(x = Year_Built, y = Total_Bsmt_SF, colour = Bldg_Type)) + 
  geom_point() + 
  labs(
    subtitle = "Total basement area per building type\nbetween 1885 and 2009",
    x = "Year built", 
    y = "Total basement area", 
    colour = "Building Type"
  ) + 
  theme_classic()
  
# As one can see it is difficult to detect a trend in the relationship...
# between Year_Built and Total_Bsmt_SF from a raw scatter plot. 
# Since there are vertically overlapping points, we can compare to average...
# Total_Bsmt_SF for each Bldg_Type built in a given year. 

ave_by_year_built <- Ames2 %>% 
  group_by(Year_Built, Bldg_Type) %>% 
  summarise(mean(Total_Bsmt_SF)) 

figure_4 <- ave_by_year_built %>% 
  ggplot(aes(x = Year_Built, y = `mean(Total_Bsmt_SF)`, colour = Bldg_Type)) + 
  geom_point() + 
  labs(
    subtitle = "Average total basement area per building type\nbetween 1885 and 2009",
    x = "Year built", 
    y = "Average total basement area", 
    colour = "Building type"
  ) + 
  theme_classic()

figure_4

# The plot shows the basement size for all building types over time...
# in Ames Iowa, has gradually increased.
# Between 1930 and 1935 the rate of basement construction appears...
# to have slown down i.e., the number of basements constructed... 
# is at a lower frequency than before and after this period.
# Reason: This period also included the Great Depression from 1929 to 1933...
# so most businesses and people did not have the means for land acquisition or
# construction. 
# The same occurs between 2007 and 2010 which coincides with...
# the 2007-2008 global financial crisis.

# (Link to sources to support these inferences are below.)

```
  Sources: 
  
  1. [Financial Crisis](https://en.wikipedia.org/wiki/Financial_crisis_of_2007%E2%80%932008)
  2. [List of Recessions in the United States](https://en.wikipedia.org/wiki/List_of_recessions_in_the_United_States)


e.  Why do we make these plots? Comment on your findings from these
    plots (1 sentence is fine). (2 points)
    
```{r}

# The box plots are to compare the distribution of numeric values...
# in each category. The scatter plot was to determine a trend...
# between the two variables and decide which model may be best fit...
# for prediction i.e., if there is a linear trend this may indicate...
# a linear regression model would best fit the data set.

# Findings: 
# - Generally between 1885 and 2009 the basement size has increased.
# - The number of TwoFmCon constructed completely stopped around 1975 and...
#   around the time Twnhs and TwnhsE basements began to be constructed.
# - Twnhs has the lowest median total basement area with 50% of its areas...
#   ranging between 550 to 850 ft^2. 
# - Duplex has the highest median total basement area with 50% of its... areas
#   areas ranging between 1100 to 1700 ft^2. This is the largest spread...
#   in the data set despite TwnshE having more outliers.

```


f.  Now choose an appropriate plot to investigate the relationship
    between Bldg_Type and Year_Built in Ames2. Why should we consider
    this? What do you notice? (3 points)
    
```{r}
# This must be considered to compare frequency of each category...
# at different periods of time.

# We should consider this to avoid over generalizing which...
# could increase the residual error of predictions due to not taking...
# the different frequencies (and therefore sample sizes)...
# of each category into account. 

# For example if we wanted to investigate Total_Bsmt_SF...
# based on Year_Built and Blg_Type between 1980 and 2008, we now know...
# we should drop the 'TwoFmCon' level because there were zero TwoFmCon...
# buildings constructed during that time period. 
# Duplex could also be dropped if the frequency...
# is found to be too small i.e., <30 or small relative to the...
# frequencies of other categories.

# We notice that the 'TwoFmCon' building was only constructed between...
# 1885 to 1965. This is around the same time, Twnhs and TwnhsE building...
# types began construction and at a noticeably higher frequency.
# Duplex building were dispersed more consistently between ...
# 1900 and 1990. 

figure_5 <- Ames2 %>% 
  ggplot(aes(x = Year_Built, fill = Bldg_Type)) +
  geom_bar(data = select(Ames2, !Bldg_Type), fill = "grey85") +
  geom_bar() + 
  facet_wrap(facets = vars(Bldg_Type)) +
  labs(
    subtitle = "Frequency of building type between 1885 and 2009",
    x = "Year built", 
    y = "Frequency", 
    fill = "Building type"
  ) +
  theme_classic() + 
  theme(
    panel.grid.major.x = element_blank(), 
    panel.grid.minor.x = element_blank(),
    panel.grid.major.y = element_line(colour = "grey95"), 
    panel.grid.minor.y = element_blank(),
    panel.background   = element_blank(),
    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)
  )
  
figure_5
# TO SEE A CLEARER VERSION OF THIS PLOT PLEASE OPEN IT IN THE .RMD FILE.
```
    

g.  Use the lm command to build a linear model, linmod1, of Total_Bsmt_SF
    as a function of the predictors Bldg_Type and Year_Built for the
    "Ames2" dataset. (2 points)
```{r}

linmod1 <- lm(formula = Total_Bsmt_SF ~ Bldg_Type + Year_Built, 
              data = Ames2)
```

h.  State and evaluate the assumptions of the model. (6 points)
```{r}

# Scatter plot of Year Built vs. Total_Bsmt_SF with linear trend line.
# The first one shows the overall trend line of all categories.
# The second one shows the separate trend line of each category. 

figure_6a <- ave_by_year_built %>% 
  ggplot(aes(x = Year_Built, y = `mean(Total_Bsmt_SF)`, colour = Bldg_Type)) + 
  geom_point() + 
  labs(
    subtitle = "Average total basement area per building type\nbetween 1885 and 2009",
    x = "Year built", 
    y = "Average total basement area", 
    colour = "Building type"
  ) + 
  geom_smooth(method = "lm", se = FALSE, colour = "black") + 
  theme_classic()

figure_6a

figure_6b <- ave_by_year_built %>% 
  ggplot(aes(x = Year_Built, y = `mean(Total_Bsmt_SF)`, colour = Bldg_Type)) + 
  geom_point() + 
   facet_wrap(facets = vars(Bldg_Type)) + 
  labs(
    subtitle = "Average total basement area per building type\nbetween 1885 and 2009",
    x = "Year built", 
    y = "Average total basement area", 
    colour = "Building type"
  ) + 
  geom_smooth(method = "lm", se = FALSE, colour = "black") + 
  theme_classic() + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))

figure_6b

figure_7 <- linmod1 %>% 
  gg_diagnose(max.per.page = 1)
# The assumption is the model follows the assumptions...
# of a Gaussian distribution that is:

# 1) Linearity -> If there is 1 categorical variable and 1 or more...
#                 continuous variables linearity is assessed by...
#                 a scatter plot with the levels in different colors...
#                 hence figure_6a and figure_6b. 
#                 Both figures show the trend is roughly...
#                 linear so this assumption is held. 

##### All plots referred to in 2) and 3) are in figure_7. #####

# 2) Normality -> Aside from a few outliers in the histogram of...
#                 of residuals it appears normality assumption is...
#                 is upheld. The normal...
#                 QQ plot also shows this since most of the standardized...
#                 residuals are along the red line.
#   
# 3) Homoscedasticity -> The model appears to violate this assumption.
#                        This is because the residuals in the plot...
#                        of Year Built vs Residuals show as the...
#                        Year Built increases the further the residuals...
#                        are from the zero-mean. A similar trend was...
#                        observed in Fitted Values vs Residuals. 
#                        The boxplot of Bldg_Type vs Residuals also...
#                        show the IQR are not similar and every category...
#                        aside from 'Duplex' are far from the zero mean.

```

i.  Use the lm command to build a second linear model, linmod2, for
    Total_Bsmt_SF as a function of Bldg_Type, Year_Built and Lot_Area.
    (2 points)
```{r}

# I assumed the question meant to the model using the...
# Ames2 dataset as well.
linmod2 <- lm(formula = Total_Bsmt_SF ~ Bldg_Type +  Year_Built + Lot_Area, 
              data = Ames2)
```
    

j.  Use Anova and Adjusted R-squared to compare these two models, and
    decide which is a better model. (6 points)
```{r}

summary(linmod1)
summary(linmod2)
anova(linmod1, linmod2)

# Anova -> 8.099e-05 < 0.05 therefore linmod2 is a more statistically...
#          significant model compared to linmod1.
#          In context, this means adding the 'Lot_Area' predictor...
#          had a significant effect on the 'Total Basement Area in...
#          Square Feet'.
# Adjusted R-squared -> linmod1 has an Adjusted R-squared value...
#                       of 0.3282 whereas linmod2 has an...
#                       Adjusted R-squared value of 0.3489. 
#                       Therefore linmod2 has a slightly better fit...
#                       than linmod1 as an additional 2.07% of the...
#                       variation in Total_Bsmt_SF can be explained by...
#                       its predictors. 
# Therefore the better model is linmod2. That said the Adjusted R-squared...
# value is still low so its predictors are explanatory, there are...
# alot of uncontrolled factors that affect the Total_Bsmt_SF.

```

k.  Construct a confidence interval and a prediction interval for the
    basement area of a Twnhs built in 1980, with a lot Area of 7300.
    Explain what these two intervals mean. (6 points)

```{r}

# Confidence interval 
predict(linmod2, 
        newdata = data.frame(Bldg_Type = "Twnhs",
                             Year_Built = 1980, 
                             Lot_Area = 7300), 
                             interval = "confidence")
# Prediction interval 
predict(linmod2, 
        newdata = data.frame(Bldg_Type = "Twnhs",
                             Year_Built = 1980, 
                             Lot_Area = 7300), 
                             interval = "prediction")
# Explanation: 
# The confidence interval is the uncertainty around the mean at each ...
# total basement area. The expected values of total basement area...
# are expected lie within this interval.
# Whereas the prediction interval is the range of likely values...
# the expected value of the total basement area could take.

```

l.  Now build a linear mixed model, linmod3, for Total_Bsmt_SF as a function of Year_Built, MS_Zoning and Bldg_Type. Use Neighborhood as random effect. What is the critical number to pull out from this, and what does it tell us? (4 points)

```{r}

linmod3 <- lmer(formula = Total_Bsmt_SF ~ 
                  Year_Built + MS_Zoning + Bldg_Type + (1|Neighborhood), 
                data = Ames2)
# Critical number: 187.4
# What it tells us: This is the standard deviation of the effect...
# 'Neighborhood' has on Year_Built and MS_Zoning. In other words...
# Neighbourhood causes 187.4 variance between the Year_Built and...
# MS_Zoning variables.

```

m.  Construct 95% confidence intervals around each parameter estimate
    for linmod3. What does this tell us about the significance of the random effect? (3 points)
```{r}

confint(linmod3)

# .sig01 = Is the random effect.
# Since the range of the confidence interval does not include 0...
# the random effect is significant.

```


n.  Write out the full mathematical expression for the model in linmod2
    and for the model in linmod3. Round to the nearest integer in
    all coefficients with modulus \> 10 and to three decimal places for coefficients with modulus \< 10. (4 points)
    
```{r}
linmod2
```
    
```{r}
linmod3
```
    

Mathematical expression for linmod2:
$$ E({ \rm TotalBsmtSF}) = -11760 + (6.509 \times {\rm YearBuilt}) + (0.008 \times {\rm LotArea}) $$ 
$$ + (238 \times {\rm isDuplex}) + (-412 \times {\rm isTwnhs}) + (-127 \times {\rm isTwnhsE})  $$ 

Mathematical expression for linmod3:
$$ E({ \rm TotalBsmtSF}) = -4981 + (2.876 \times {\rm YearBuilt})+ (149 \times {\rm isHighDensity}) + (288 \times {\rm isLowDensity})$$
$$ + (109 \times {\rm isMediumDensity}) + (265 \times {\rm isDuplex}) + (-63 \times {\rm isTwnhs}) + (105 \times {\rm isTwnhsE}) + U $$
$$ U \sim N(0, 187)$$
$$ {\rm TotalBsmtSF} \sim N(E({\rm TotalBsmtSF}), 262)$$ 

# 3. Logistic Regression

a.  Do the following:
  (i) Create a new dataset called "Ames3" that contains all data in "Ames" dataset plus a new variable "excellent_heating" that indicates if the heating quality and condition "Heating_QC" is excellent or not. (2 points)
  
```{r}

Ames3 <- Ames %>% 
  mutate(
    excellent_heating = if_else(Heating_QC == "Excellent", "Yes", "No")
  ) 

# The code below checks whether this new variable is correct.
Ames3 %>% 
  summarise (
    Heating_QC, 
    excellent_heating
  )

```
  
  (ii) In "Ames3" dataset, remove all cases "3" and "4" corresponding to the Fireplaces variable. Remove all cases where Lot_Frontage is greater than 130 or smaller than 20. Drop the unused levels from the dataset. (2 points)
  
```{r}

Ames3 <- Ames3 %>% 
  filter(Fireplaces != 3 & Fireplaces != 4) %>% 
  filter(Lot_Frontage >= 20 & Lot_Frontage <= 130) %>% 
  droplevels()

```
  
 (iii) Save "Fireplaces" as factor in "Ames3" dataset (1 point)
```{r}

Ames3 <- Ames3 %>% 
  mutate (
    Fireplaces = as.factor(Fireplaces)
  )

```
 
 (iv) Construct a logistic regression model glmod for excellent_heating as a function of Lot_Frontage and Fireplaces for the dataset "Ames3". (2 points)

```{r}

glmod <- glm(formula = as.factor(excellent_heating) ~ Lot_Frontage + Fireplaces, 
             family = "binomial",
             data = Ames3)
glmod
```


b.  Construct confidence bands for the variable excellent_heating as a function of Lot_Frontage for each number of Fireplaces (hint:
    create a new data frame for each number of Fireplaces). Colour these with different transparent colours for each number of Fireplaces and plot them together on the same axes. Put the actual data on the plot, coloured to match the bands, and jittered in position to make it possible to
    see all points. Ensure you have an informative main plot title, axes labels and a legend. (7 points)

```{r}

ilink <-family(glmod)$linkinv

newf <- with(
  Ames3,
  data.frame(
    Lot_Frontage = seq(min(Ames3$Lot_Frontage), 
                      max(Ames3$Lot_Frontage), 
                      length = 100), 
    Fireplaces))

newf <- cbind(newf,
              predict(
                glmod,
                newf,
                type ="link",
                se.fit=TRUE)[1:2])

newf <-transform(newf,
                 Fitted = ilink(fit),
                 Upper = ilink(fit+(1.96*se.fit)),
                 Lower = ilink(fit-(1.96*se.fit)))

#### Original (actual data without jitter) ####
figure_8a <- ggplot(Ames3,
                   aes(x = Lot_Frontage,
                       y = as.numeric(as.factor(excellent_heating)) - 1, 
                       colour = Fireplaces)) + # so the points are coloured.
  geom_ribbon(data = newf, 
              aes(
                ymin = Lower,
                ymax = Upper,
                x = Lot_Frontage,
                fill = Fireplaces,
                colour = Fireplaces),
              alpha = 0.2,
              inherit.aes = FALSE) +
    geom_line(data = newf, 
              aes(y = Fitted,
                  x = Lot_Frontage,
                  group = Fireplaces,
                  colour = Fireplaces)) +
  geom_point(alpha = 0.5) + # so all points can be seen.
  labs(
    title = "Without jitter",
    subtitle = "The probability and corresponding confidence\ninterval of
    Lot Frontage obtaining an excellent rating\n based on each Fireplace type",
    x = "Lot Frontage",
    y = "Probability of excellent heating") + 
  theme_classic()

figure_8a
# TO SEE A CLEARER VERSION OF THIS PLOT PLEASE OPEN IT IN THE .RMD FILE.

#### Final (actual data with jitter) ####
figure_8b <- ggplot(Ames3,
                   aes(x = Lot_Frontage,
                       y = as.numeric(as.factor(excellent_heating)) - 1, 
                       colour = Fireplaces)) + # so the points are coloured.
  geom_ribbon(data = newf, 
              aes(
                ymin = Lower,
                ymax = Upper,
                x = Lot_Frontage,
                fill = Fireplaces,
                colour = Fireplaces),
              alpha = 0.2,
              inherit.aes = FALSE) +
    geom_line(data = newf, 
              aes(y = Fitted,
                  x = Lot_Frontage,
                  group = Fireplaces,
                  colour = Fireplaces)) +
  geom_point(position = "jitter", alpha = 0.5) + # so all points can be seen.
  labs(
    title = "With jitter",
    subtitle = "The probability and corresponding confidence\ninterval of
    Lot Frontage obtaining an excellent\nrating based on each Fireplace type",
    x = "Lot Frontage",
    y = "Probability of excellent heating") + 
  theme_classic()

figure_8b
# TO SEE A CLEARER VERSION OF THIS PLOT PLEASE OPEN IT IN THE .RMD FILE.
```


c.  Split the data using set.seed(120) and rebuild the model on 80% of the data. Cross validate on the remaining 20%. Plot the ROCs for
    both data and comment on your findings. (6 points)

```{r, results = "hide"}

set.seed(120)
training.samples <- c(Ames3$excellent_heating) %>%
  createDataPartition(p = 0.8, list = FALSE)
train.data <- Ames3[training.samples, ]
test.data <- Ames3[-training.samples, ]

train.model <- glm(formula = as.factor(excellent_heating) ~ Lot_Frontage + Fireplaces, 
             family = "binomial",
             data = train.data)

predtrain <- predict(train.model, type = "response")
predtest <- predict(train.model, newdata = test.data , type = "response")


roctrain <- roc(response = train.data$excellent_heating,
                predictor = predtrain,
                plot = TRUE,
                main = "ROC Curve for prediction of a\n'Yes' excellent rating",
                cex.main = 0.85, 
                auc = TRUE)

roc(response = test.data$excellent_heating,
    predictor = predtest,
    plot = TRUE,
    auc = TRUE,
    add = TRUE,
    col = 2)

legend(0, 0.4, legend = c("Training", "Testing"), fill = 1:2, cex = 0.4)

# The testing and training ROC curves are similar in shape and closely overlap.
# Therefore this is a good indication the testing data is not...
# over fitted to the training data.

```


# 4. Multinomial Regression

a.  For the dataset "Ames", create a model multregmod to predict BsmtFin_Type_1 from Total_Bsmt_SF and Year_Remod_Add. (3 points)

```{r}

multregmod <- multinom(formula = BsmtFin_Type_1 ~ Total_Bsmt_SF + Year_Remod_Add, 
                       data = Ames)
multregmod

```


b.  Write out the formulas for this model in terms of P(No_Basement), P(Unf) P(Rec),P(BLQ), P(GLQ), P(LwQ),  
   You may round coefficients to 3 dp. (4 points)
   
The probabilities in terms of logit are:

$$ logit(P({\rm BLQ})) = 34.465 + ((6.283 \times 10^{-5}) \times {\rm TotalBmstSF}) + (-0.018 \times {\rm YearRemodAdd}) $$
$$ logit(P({\rm GLQ})) = -105.324 + (0.001 \times {\rm TotalBmstSF}) + (0.053 \times {\rm YearRemodAdd}) $$
$$ logit(P({\rm LwQ})) = 39.567 + ((1.244 \times 10^{-5}) \times {\rm TotalBmstSF}) + (-0.021 \times {\rm YearRemodAdd}) $$
$$ logit(P({\rm NoBasement})) = 4.876 + (-0.173 \times {\rm TotalBmstSF}) + (0.004 \times {\rm YearRemodAdd}) $$
$$ logit(P({\rm Rec})) = 56.711 + ((1.597 \times 10^{-6}) \times {\rm TotalBmstSF}) + (-0.029 \times {\rm YearRemodAdd}) $$
$$ logit(P({\rm Unf})) = -29.377 + ((-6.99 \times 10^{-4}) \times {\rm TotalBmstSF}) + (0.016 \times {\rm YearRemodAdd}) $$

Where: 

```{r}
# Checked levels to find the base case.
levels(Ames$BsmtFin_Type_1)
```

$$ P({\rm ALQ}) = 1 - P({\rm BLQ}) - P({\rm GLQ}) = P({\rm LwQ}) - P({\rm NoBasement})\\ - P({\rm Rec}) - P({\rm Unf})$$
   
c.  Evaluate the performance of this model using a confusion matrix and
    by calculating the sum of sensitivities for the model. Comment on
    your findings. (4 points)

```{r}
multitable <- table(Ames$BsmtFin_Type_1,
                    predict(multregmod,
                            type ="class"))

names(dimnames(multitable)) <- list("Actual","Predicted")

multitable
```

Evaluation: 

First calculate the sensitivity and specificity for each category:
$$ Sensitivity({\rm ALQ}) = \frac{1}{1 + 117 + 18 + 293} = 0.233\%$$

$$ Sensitivity({\rm BLQ}) = \frac{0}{50 + 30 + 189} = 0\% $$

$$ Sensitivity({\rm GLQ}) = \frac{579}{1 + 579 + 2 + 277} = 67.404\% $$

$$ Sensitivity({\rm LwQ}) = \frac{0}{1 + 38 + 30 + 85} = 0\% $$

$$ Sensitivity({\rm NoBasement}) = \frac{80}{80} = 100\%$$

$$ Sensitivity({\rm Rec}) = \frac{46}{3 + 31 + 46 + 208} = 15.972\% $$

$$ Sensitivity({\rm Unf}) = \frac{478}{6 + 291 + 76 + 478} = 56.169\%$$


2. Then calculating the sum of sensitivities for the model: 
```{r}
# Calculated manually:
sum_of_sensitivites <- 0.002331002 + 0.6740396 + 1 + 0.1597222 + 0.5616921

sum_of_sensitivites

# Checked by automated sum of sensitivities: 
ss_check <- multitable[1,1]/sum(Ames$BsmtFin_Type_1=="ALQ") + 
            multitable[2,2]/sum(Ames$BsmtFin_Type_1=="BLQ") + 
            multitable[3,3]/sum(Ames$BsmtFin_Type_1=="GLQ") + 
            multitable[4,4]/sum(Ames$BsmtFin_Type_1=="LwQ") +
            multitable[5,5]/sum(Ames$BsmtFin_Type_1=="No_Basement") +
            multitable[6,6]/sum(Ames$BsmtFin_Type_1=="Rec") +
            multitable[7,7]/sum(Ames$BsmtFin_Type_1=="Unf")

ss_check
# Since these two values are the same the sum of the sensitivities is correct.

# Comments: Sensitivities per category -> 
# Generally the model does a better job at predicting GLQ, No_Basement and Unf. 
# Particularly No_Basement which predictions were correct 100% of the time.
# In comparison ALQ and Rec were much less likely to be predicted correctly, 
# with sensitivities of 0.002331002 and 0.1597222 respectively.
# The model was worst at predicting BLQ and LwQ with sensitivities of 0.

# Comments: Sum of sensitivities -> 
# The sum of the sensitivities is 2.397785. 
# This is less than 7 which would be the sum of sensitivities...
# for a perfect model. The model is not very good overall...
# since the sum of sensitivities is well below the...
# perfect model value. Instead only 34% (2.s.f)...
# of all categories are correctly predicted overall.

```


# 5. Poisson/quasipoisson Regression

a.  For the "footballer_data" dataset, create a model appearances_mod to predict the
    total number of overall appearances a player had based on position
    and age. (2 points)

```{r}
appearances_mod <- glm(formula = appearances_overall ~ position + age,
                       data = footballer_data,
                       family = "poisson")

```

b.  Check the assumption of the model using a diagnostic plot and
    comment on your findings. (3 points)

```{r}
# The assumption that needs to be checked is whether the variance = mean...
# i.e., the dispersion assumption.

plot(appearances_mod, which = 3)
abline(h = 0.8, col = 3)

# Findings: 
# The red line is not flat, and it is consistently above 0.8.
# This suggests over dispersion in the data that decreases...
# somewhat linearly as the prediction increases. Therefore...
# the variance is not equal to the mean.

# Since the dispersion appears to roughly be a linear function...
# of the mean, it is possible a quasipoisson model may...
# be a better fit and reduce the magnitude of over dispersion.

# This was tested below: 
appearances_mod2 <- glm(formula = appearances_overall ~ position + age,
                       data = footballer_data,
                       family = "quasipoisson")

plot(appearances_mod2, which = 3)
abline(h = 0.8, col = 3)

# And as expected the over dispersion reduces. This indicates the...
# the quasipoisson is more suitable than the poisson model.
```


c.  What do the coefficients of the model tell us about? which position
    has the most appearances? How many times more appearances do
    forwards get on average than goalkeepers? (3 points)
    
```{r}
# Used the quasipoission model (since it has less over dispersion):
summary(appearances_mod2)

# The coefficients of the model tells us...
# Age: Every increase of appearances overall by 1...
#      is due to an increase in age by a rate of 0.043704.
# Position: And it tells us the rate of change of each...
#           positions relative to the base case position - the Defender.

# Position with the most appearances: Midfielder

# How many more times forwards appear than goalkeepers: 1.6 times
# Rounded up its 2 times.
forward_goalkeeper_ratio <- exp(0.110606)/exp(-0.364605)
forward_goalkeeper_ratio

```
    
    